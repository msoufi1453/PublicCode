{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "239c8c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed051a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#setup globabl variables\n",
    "words = open('names.txt','r').read().splitlines()\n",
    "chars = sorted(list(set(''.join(words)))) #list of chars\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)}#mapping of chars to integers\n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s,i in stoi.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c70df61c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#build the dataset\n",
    "block_size = 3\n",
    "X, Y = [], []\n",
    "#X is embedding of 3 characters\n",
    "#Y is output after the 3 characters\n",
    "\n",
    "for word in words:\n",
    "    context = [0]*block_size\n",
    "    #print(word)\n",
    "    for char in word + '.':\n",
    "        ix = stoi[char]\n",
    "        X.append(context)\n",
    "        Y.append(ix)\n",
    "        #print(''.join(itos[i] for i in context), '--->', itos[ix])\n",
    "        context = context[1:] + [ix] #crop and append\n",
    "X = torch.tensor(X)\n",
    "Y = torch.tensor(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d995c2",
   "metadata": {},
   "source": [
    "## build a neural net to predict letter from Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "12f779e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.Generator().manual_seed(2147483647)\n",
    "C = torch.randn((27,2), generator=g) #each letter is a 2D vector\n",
    "\n",
    "#build a 2 layer neural net to predict Ys from Xs\n",
    "#first layer goes to 100\n",
    "W1 = torch.randn((6,100), generator=g) #weights\n",
    "b1 = torch.randn(100, generator=g) #biases\n",
    "\n",
    "#condense 100 to 27 digits\n",
    "W2 = torch.randn((100, 27), generator=g) #weights\n",
    "b2 = torch.randn(27, generator=g) #biases\n",
    "\n",
    "parameters = [C, W1, b1, W2, b2]\n",
    "for p in parameters:\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a09ce8f6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.9306527376174927\n"
     ]
    }
   ],
   "source": [
    "for _ in range(10000): \n",
    "    #minibatch\n",
    "    ix = torch.randint(0, X.shape[0], (32,))\n",
    "    \n",
    "    #forward pass \n",
    "    emb = C[X[ix]]\n",
    "    h = torch.tanh(emb.view(-1, 6)@W1 + b1)\n",
    "    logits = h@W2 + b2\n",
    "    #calculate loss\n",
    "    loss = F.cross_entropy(logits, Y[ix]) #F.cross_ejntropy computes loss\n",
    "    \n",
    "    #backward pass\n",
    "    for p in parameters:\n",
    "        p.grad = None # zero out params\n",
    "    loss.backward()\n",
    "    #update\n",
    "    for p in parameters:\n",
    "        p.data += -0.1*p.grad\n",
    "\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e679d54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#finding optimal learning rate (we found it was 0.1)\n",
    "rates = torch.logspace(-3, 0, 100)\n",
    "losses = []\n",
    "for i, rate in enumerate(rates):\n",
    "    for _ in range(10): \n",
    "        #minibatch\n",
    "        ix = torch.randint(0, X.shape[0], (32,))\n",
    "        \n",
    "        #forward pass \n",
    "        emb = C[X[ix]]\n",
    "        h = torch.tanh(emb.view(-1, 6)@W1 + b1)\n",
    "        logits = h@W2 + b2\n",
    "        #calculate loss\n",
    "        loss = F.cross_entropy(logits, Y[ix]) #F.cross_ejntropy computes loss\n",
    "        \n",
    "        \n",
    "        #backward pass\n",
    "        for p in parameters:\n",
    "            p.grad = None # zero out params\n",
    "        loss.backward()\n",
    "        #update\n",
    "        for p in parameters:\n",
    "            p.data += -0.1*p.grad\n",
    "        losses.append(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2713fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ideally you want to shuffle the data set mutliple times to have \n",
    "#80% in training set, 10% to search for hyperparameters, 10% for validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c1ac6894",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dex.',\n",
       " 'maidalluraile.',\n",
       " 'kayda.',\n",
       " 'kazimi.',\n",
       " 'tain.',\n",
       " 'lunan.',\n",
       " 'kaida.',\n",
       " 'jamivaulla.',\n",
       " 'srigot.',\n",
       " 'jeig.',\n",
       " 'jellavo.',\n",
       " 'jaiteda.',\n",
       " 'kalemka.',\n",
       " 'sade.',\n",
       " 'ankavirny.',\n",
       " 'fols.',\n",
       " 'mhina.',\n",
       " 'lavtahlas.',\n",
       " 'kasd.',\n",
       " 'del.']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sample from model\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "names = []\n",
    "for _ in range(20):\n",
    "    out=[]\n",
    "    context = [0]*block_size\n",
    "    while True:\n",
    "        emb = C[torch.tensor([context])] #create embedding from context\n",
    "        h = torch.tanh(emb.view(1, -1) @ W1 + b1)\n",
    "        logits = h @ W2 + b2\n",
    "        probs = F.softmax(logits, dim=1) #find probabiltiy distribution from logits\n",
    "        ix = torch.multinomial(probs, num_samples=1, generator=g).item() #sample\n",
    "        context = context[1:] + [ix] #update context\n",
    "        out.append(ix)\n",
    "        if ix == 0:\n",
    "            break\n",
    "    names.append(''.join(itos[i] for i in out))\n",
    "names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1c3acb8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 5, 12, 0]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9bc980",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
